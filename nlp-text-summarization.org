#+TITLE: NLP Text Summarization
#+ATTR_LaTeX: :mode inline-math :environment array :align left
#+STARTUP: inlineimages


* Text Summarization

Summarization is the task of producing a shorter version of a document that preserves most of the original document's meaning.

* Text Summarization of this post
This post is rather long.  Let me text summarize it for you.

Here are a few results

Predicting text of pneumonia:
Predicting category of text about abortion:
Naive summarization: 
Google example summarizations with tensorflow:
Google example of summarization from paper: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43852.pdf (Sentence Compression by Deletion with LSTMs)

Now you have two options read this or jump directly to the resources section.

There is another concept called ~Sentence Compression~ which is taking a large sentence possibly multiple but few and shortening it.


* Disclosure

I just began to study this topic, most of the things i'm either talking about or practicing here i'm doing for the first time.  I'm by no means an expert and not even a novice :).  There would be mistakes in that article.  I will share my learning and discoveries with you.  Document is in progress and would get updated.

* Our plan

1. Introduction - meet text summarization.
1. The jargon.
1. Two published researches.
1. History of text summarization.
1. The different methods.
1. Let's write some code.
1. Where do I plan to head on.
1. Summary.

* Jargon

I always find that in any topic I study the jargon/taxonomy/terminology is one of the most important things to know so here it is:

|------------------------+--------------------------------------------------------------------------------------------------------------------------|
| Term                   | Description                                                                                                              |
|------------------------+--------------------------------------------------------------------------------------------------------------------------|
| Text Summarization     | Computer creating meaninful summaries of text                                                                            |
| Sentence Compression   | Take a large sentence(or few) and compress                                                                               |
| Google knowledge graph | A enhancement to search where it shows informative data on right panel for search results                                |
|                        | ex. [[https://www.google.com/intl/es419/insidesearch/features/search/assets/img/snapshot.jpg]]                               |
| Ontology               | Doamin specific information: "The ontologies on the Web range from large taxonomies categorizing                         |
|                        | Web sites (such as on Yahoo!) to categorizations of products for sale and their features                                 |
|                        | (such as products on Amazon.com)" http://vladimiralexiev.github.io/pres/20160329-OpenData-and-Ontologies/index-full.html |
|                        | Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic |
|                        | relationships between the concepts that these terms represent from a corpus of natural language text                     |
|                        | creation of ontologies, including extracting the corresponding domain's terms.                                           |
| NLG                    | Natural Language Generation                                                                                              |
|                        |                                                                                                                          |


* Introduction

What is text summarization: An example could be great here so below is a real world one:

Article:

#+BEGIN_QUOTE
novell inc. chief executive officer eric schmidt has been named chairman of the internet search-engine company google .
#+END_QUOTE

Human Summary:

#+BEGIN_QUOTE
novell ceo named google chairman
#+END_QUOTE

Machine Summary:

#+BEGIN_QUOTE
novell chief executive named to head internet company
#+END_QUOTE

Reference: [[https://github.com/tensorflow/models/tree/master/research/textsum][TensonFlow Research Text Summarization]]

Yes, most text summarization train data, research and example models are focused on news, if you are not in news business most chances you need to get your own data and retrain, no ready models for you.

How do we (humans, although  some bots are also reading this..) summarize text?

1. We read it mostly or fully, we had better do that if we want a good summary.
1. We understand it's meaning.
1. We understand it's context.
1. We read or already have read similar documents.
1. We think about it.
1. We put in intuitiveness.
1. We put in knowledge gathered over the years.
1. We apply templates.
1. We know or assume we know what our audience expects.
1. We highlight important phrases, words, sentences.
1. We possibly sleep on it to get some more ideas.
1. We do more things but I had to stop at some point.

And then:

**We come up with a much shorter version of the orig doc which contains the main ideas and shares the intent presented in the original doc - the glorious summary**

or as "Text Summarization Techniques" paper says:

#+BEGIN_QUOTE
a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually, significantly less than that
#+END_QUOTE

How do they (computers) summarize text?

Who knows!

Can they do that?
If yes can they do that in a satisfactory manner?
Will they have embarrassing mistakes?
How far are they from humans? Or maybe how advanced?

Well, i'm not sure there are answers to all the above questions, but, let's find out together, this is what we are here for.

* First Paper - Text Summarization Techniques

[[https://arxiv.org/abs/1707.02268][Text Summarization Techniques: A Brief Survey]]

This is the paper that we need to get started, their premise is: 

#+BEGIN_QUOTE
We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.
#+END_QUOTE

Sounds promising for a start, in addition quickly scanning the paper I see no math formula, for me this means I'll be able to finish this paper in less than a year :) Yay!

* Updated plan

* The importance of ontology

If you take a look at leonardo right hand side summary in wikipedia or google summary you would see they managed to extract the relevant properties and fill it in.

* Naive Code

**Note, in our use case we are less interested in human complete sentences but more of a few words together which summarize the topic.**

In our first naive code implementation here is our plan, no machine learning, just take some text and try to summarize it, somehow.  Let's see:

Step 1: Here is our bunch of text to summarize:

#+BEGIN_SRC python

text: str = """
Leonardo da Vinci
Leonardo di ser Piero da Vinci (Italian: [leoˈnardo di ˌsɛr ˈpjɛːro da (v)ˈvintʃi] (About this sound listen); 15 April 1452 – 2 May 1519), more commonly Leonardo da Vinci or simply Leonardo, was an Italian polymath of the Renaissance, whose areas of interest included invention, painting, sculpting, architecture, science, music, mathematics, engineering, literature, anatomy, geology, astronomy, botany, writing, history, and cartography. He has been variously called the father of palaeontology, ichnology, and architecture, and is widely considered one of the greatest painters of all time. Sometimes credited with the inventions of the parachute, helicopter and tank,[1][2][3] he epitomised the Renaissance humanist ideal.

Many historians and scholars regard Leonardo as the prime exemplar of the "Universal Genius" or "Renaissance Man", an individual of "unquenchable curiosity" and "feverishly inventive imagination",[4] and he is widely considered one of the most diversely talented individuals ever to have lived.[5] According to art historian Helen Gardner, the scope and depth of his interests were without precedent in recorded history, and "his mind and personality seem to us superhuman, while the man himself mysterious and remote".[4] Marco Rosci notes that while there is much speculation regarding his life and personality, his view of the world was logical rather than mysterious, and that the empirical methods he employed were unorthodox for his time.[6]

Born out of wedlock to a notary, Piero da Vinci, and a peasant woman, Caterina, in Vinci in the region of Florence, Leonardo was educated in the studio of the renowned Florentine painter Andrea del Verrocchio. Much of his earlier working life was spent in the service of Ludovico il Moro in Milan. He later worked in Rome, Bologna and Venice, and he spent his last years in France at the home awarded to him by Francis I of France.

Leonardo was, and is, renowned primarily as a painter. Among his works, the Mona Lisa is the most famous and most parodied portrait[7] and The Last Supper the most reproduced religious painting of all time.[4] Leonardo's drawing of the Vitruvian Man is also regarded as a cultural icon,[8] being reproduced on items as varied as the euro coin, textbooks, and T-shirts.

A painting by Leonardo, Salvator Mundi, sold for a world record $450.3 million at a Christie's auction in New York, 15 November 2017, the highest price ever paid for a work of art.[9] Perhaps fifteen of his paintings have survived.[nb 1] Nevertheless, these few works, together with his notebooks, which contain drawings, scientific diagrams, and his thoughts on the nature of painting, compose a contribution to later generations of artists rivalled only by that of his contemporary, Michelangelo.

Leonardo is revered for his technological ingenuity. He conceptualised flying machines, a type of armoured fighting vehicle, concentrated solar power, an adding machine,[10] and the double hull. Relatively few of his designs were constructed or even feasible during his lifetime, as the modern scientific approaches to metallurgy and engineering were only in their infancy during the Renaissance. Some of his smaller inventions, however, such as an automated bobbin winder and a machine for testing the tensile strength of wire, entered the world of manufacturing unheralded. A number of Leonardo's most practical inventions are nowadays displayed as working models at the Museum of Vinci. He made substantial discoveries in anatomy, civil engineering, geology, optics, and hydrodynamics, but he did not publish his findings and they had no direct influence on later science.[11]"""

#+END_SRC

Leonardo was a good man, let's naively summarize him.

First, how would you summarize this text, let's say limiting to 7 words?

I would say this: 

Human summary: "Leoardo Da Vinci, italian, renaisssane, painter, sculpturer"

Now lets move on with our naive code implementation:

Step 2: Tokenize the words:

#+BEGIN_SRC python

words = word_tokenize(text) # thanks nltk

#+END_SRC

Step 3: Score words based on their frequency

#+BEGIN_SRC python

words_score: FreqDist = FreqDist() # thanks nltk
for word in words:
    words_score[word.lower()] += 1

#+END_SRC

Step 4: The summary would be our top 7 frequent words:

#+BEGIN_SRC python

def top_scores_sorted_by_text(w_scores: FreqDist, k: int):
    return sorted(w_scores.most_common(k), key=lambda w: word_index(text, w))

summary = top_scores_sorted_by_text(words_score, 7)
print(summary)

#+END_SRC

Let's see our result

#+BEGIN_SRC python

[('[', 15), ('his', 17), (',', 67), ('of', 31), ('the', 32), ('and', 26), ('.', 21)] # that's a horrible summary!

#+END_SRC

We have ~his~ ~of~ ~the~ obviously we don't want them in our summary let's get rid of them:

Step 5: Get rid of stop words

#+BEGIN_SRC python

stop_words: Set[str] = set(stopwords.words("english")) # thanks nltk
words = [w for w in words if not w in stop_words] # thanks python
text = ' '.join(words) # and the updated text (sorry immutability) is now a join of the words without stop words.

#+END_SRC

Now let's print again the resulting summary

#+BEGIN_SRC python

[('leonardo', 11), ('da', 5), ('vinci', 6), ('[', 15), (']', 15), (',', 67), ('.', 21)]

#+END_SRC

This is somewhat a little better version we have ~leonardo da vinci~ as the first 3 words in summary sounds perfect! but we have also lot of puncutaions, let's get rid of them:

Step 6: Get rid of punctuations

#+BEGIN_SRC python

def remove_punctuations(s: str) -> str:
    table = str.maketrans({key: None for key in string.punctuation}) # standard python (thanks).
    return s.translate(table)

text = remove_punctuations(text)

#+END_SRC

And print again the summary:

#+BEGIN_SRC python

[('leonardo', 9), ('da', 5), ('vinci', 6), ('he', 4), ('renaissance', 4), ('painting', 4), ('engineering', 3)]

#+END_SRC

Uh, looks much better.  There is one issue, we have ~he~ in the summary, we don't want it, we have only 7 words and no space to waste, could it be that leonaro was proficient in another topic?

Step 7: Fix stop word bug

We have a bug, we have removed the stopwords with: ~[w for w in words if not w in stop_words]~ but somehow the ~he~ stopword has sneaked inside. Let's fix it, the problem is that we didn't lower case the text so ~He~ was not considered as the stopword ~he~

#+BEGIN_SRC python

text = text.lower() # no immutability small example.

#+END_SRC

And now let's run the summary again:

#+BEGIN_SRC python

[('leonardo', 9), ('da', 5), ('vinci', 6), ('renaissance', 4), ('painting', 4), ('engineering', 3), ('inventions', 3)]

#+END_SRC

No more ~he~ stopword.  This even looks like a much better summary that my original (human) one!

**But don't get excited, there are millions if not billions of summaries this naive dumb summarized would not pass, just think of products for sale.  If we think of products for sale we need a better flow.**

We could think of more enhancements:

1. Give higher score to words appearing in title.
1. Refer to query (if got to this page by search).
1. More..

Let's summary what we have done in the above naive summarizer:

#+BEGIN_SRC 

┌─────────────────────────────────────────────────────────────────────────────────────────────────────┐
│Text Summarization Very Naive Implementation                                                         │
│                                                                                                     │
│┌───────────────────┐      ┌───────────────────┐      ┌───────────────────┐     ┌───────────────────┐│
││                   │      │                   │      │                   │     │                   ││
││Get Some text from │      │      Cleanup      │      │   Words Scoring   │     │Select top k words ││
││     wikipedia     │─────▶│                   │─────▶│                   │────▶│  as our summaruy  ││
││                   │      │                   │      │                   │     │                   ││
│└───────────────────┘      └───────────────────┘      └───────────────────┘     └───────────────────┘│
│                                     │                          │                                    │
│                                     ▼                          ▼                                    │
│                           ┌───────────────────┐      ┌───────────────────┐                          │
│                           │Remove punctuations│      │  Frequency Table  │                          │
│                           └───────────────────┘      └───────────────────┘                          │
│                                     │                                                               │
│                                     ▼                                                               │
│                           ┌───────────────────┐                                                     │
│                           │    Lower case     │                                                     │
│                           └───────────────────┘                                                     │
│                                     │                                                               │
│                                     ▼                                                               │
│                           ┌───────────────────┐                                                     │
│                           │ Remove stopwords  │                                                     │
│                           └───────────────────┘                                                     │
└─────────────────────────────────────────────────────────────────────────────────────────────────────┘

#+END_SRC

A few points to note:

1. This is extractive text summarizer we didn't invent anything, no semantic understanding, we just selected words.
1. There is a better algorithm called ~SumBasic~

* SumBasic 


Here is the formula for sum basic:

\begin{equation}
g(S_j)=\frac{\sum_{w_i\in{S_j}}P(w_i)}{|\{w_i|w_i\in{S_j}|}
\end{equation}

This looks complex to me.  But I found that after I got what each symbol means it became simple, even embarrasingly simple.

Here is the meaning of that formula:

|----------------------------------+-----------------------------------------------------------------|
| term                             | meaning                                                         |
|----------------------------------+-----------------------------------------------------------------|
| g(S_j)                           | Weight of sentence ~j~                                          |
| w_i\in{S_j}                      | For each word that belongs to sentence j                        |
| \sum_{w_i\in{S_j}}P(w_i)         | The sum of all probabilities of words that belong to sentence j |
| {\vert\{w_i\vertw_i\in{S_j}\vert | Number of words in the sentence j                               |
|----------------------------------+-----------------------------------------------------------------|

So that turns g(S_j) to be the average probability of words in sentence j where word probabilty is simply the number of occurences of word w_i inside the document.

This is very similar to what we did with words without knowing ~SumBasic~! In our case we wanted to get a bunch of words and not a bunch of sentences so we just took the words appearing most, which is similar to taking the sentences with highest word probablity.

SumBasic then continues to update each word probability as it's multiplication by itself (reduce it) so we can now pick other sentences, and it keeps on with this loop until we picked as much sentences as we meant to.

* Updated Plan

Now that we did a variation on SumBasic for words instead of sentences, lets move on with more examples appearing on the web.  Namely algorithms that do more of understanding of the text and compose new text and not just choose and extract ready made summary from our existing text.

**Step 1: Mode: Classify text**

Is the text about an artist? is the text about a car is the text about an electric cleaning machine?

**Step 2: Manual: Idetify the main features of the topic**

That is the ontology of the topic.
We have identified that the text is about an electric cleaning machine this means, we need these features (this is the task to identify the features)

1. Watts
1. Target
1. Price
1. Size

**Step 3: Given an article identify topic fill in feature values**

So given an article identify:

1. Which topic is it about?
1. What are the features of that topic?
1. Fill in the values from the article about the features of that topic.


<---- **This is were I am now i'm trying to do the steps 1,2,3 and come up with a good summary, i will first identify the category of the text, then based on category I will identify the features, then based on text I will extract the values to fill into the template (like the right hand side google/wikipedia summary card) then this would become the summary!** so this would involve some more heavy coding and thinkin... ----->

** Extraction

 You take existing phrases from text, this is both good and bad.  Good because you don't have to do any nlg and bad because who promises a good summary is extracted from the sentences already in the document..

* Step 1: Identify Article Topic

This is also called **Text Classification**.  There 3 main categories to achieve Text Classification:

1. Rules
1. Standard Machine Learning Models
1. Deep Learning  

I don't have time for rules, my laptop is too slow for deep learning and i'm not sure I have enought data, so i'll go with option 2 standard models.

There is a great example (i'm doing this for the first time) at sklearn website for how to build a model to classify text. [[http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html]] I'm simply going to use and run it.

Creating the model and prediciting the class/topic for the article will involve the following steps:

1. Load labeled newgroups data with topics.
1. Vectorize the documents, BOW (Bag Of Words).
1. We can do better than BOW so we are going to TFIDF the docs to get the target vectors.
1. Run train
1. Predict

We are not going to check the accuracy, just run arbitrary example on the model.

Note that sklearn will handle the large sparse matrix issue (consming much of RAM) for us, it's going to shrink them automatically.  (did i say thanks sklearn?)

**Step 1: Load Labeled newsgroups data with topics**

#+BEGIN_SRC python

from sklearn.feature_extraction.text import CountVectorizer
import json

categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)
twenty_train.target_names = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']

#+END_SRC

In the above code we:

1. define our categories, we have defined 4 newsgroups categories.  Note that sklearn knows to fetch this example data automatially for us.
1. Load the text data into a variable ~twenty_train~
1. Add a new member to ~twenty_train~ named ~target_names~ with our categories.

**Step 2: Feature engineering**

We have loaded our data which is just a set of newsgroups posts.  What are it's features? It's a text data, so it has words right? so each distinct word is going to serve as a feature.  In our case BOW means a matrix where each doc is a row and each column is a word and we count the number of times such word appears in each doc.  Guess what, sklearn will do that automatically for us and also shrink the sparse matrix (most of words do not appear in each doc).

BOW code:

#+BEGIN_SRC python

count_vect = CountVectorizer() 
X_train_counts = count_vect.fit_transform(twenty_train.data) # Tokenize, Filter Stopwords, BOW Features, Transform to vetor, this returns Term Document Matrix! thanks sklearn

#+END_SRC

That's it with 2 lines we have tokenized the newgroup messages, filtered stopwords, extracted BOW features, transformed them to a vector (numbers).

BOW is skewed toward large documents where words appear more so we are going to turn our face to the TFIDF vectorizing instead of BOW, here is the code to do that:

**Step 3: Replace BOW with TFIDF**

#+BEGIN_SRC python

from sklearn.feature_extraction.text import TfidfTransformer
tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts) # Transform a count matrix to a normalized tf or tf-idf representation
X_train_tf = tf_transformer.transform(X_train_counts) # Transform a count matrix to a tf or tf-idf representation # X_train_tf.shape
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

#+END_SRC

The above code is self explanatory we first do TF and then IDF, note that we do all operatoins with just a few lines, sklearn appears to be very developer friendly and has concise and clear api, no wonder it's so common.

Now that we have our data loaded, and extracted all the features from it (vectorized with tfidf) it's time to build the model.

**Step 4: Build the model to predict class of newsgroup message**

#+BEGIN_SRC python

from sklearn.naive_bayes import MultinomialNB # Naive bayes classifier
clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)

#+END_SRC 

There are multiple classifiers we are following sklearn example, in our example, so we have chosen the same.  We then called ~fit~ and passed as input: ~X_train_tfidf~ that is the set of features for each doc (the tfidf vectors) and as the labels/output we train the model with ~twenty_train.target~ which is the vector of topics we train the model with for each row.

Now for money time, we are going to predict something, i'm going to take an arbitrary wikipedia article that deals with one of the 4 categories and see if it's well predicted, so what have we got there, science medicine, religion, computer graphics, and atheism.

To test the prediction we are not going to run on a set of artiles but just pick two example articles from wikipedia and see the outcome prediction.  At first let's pick an easy one I think, an artile from wikipedia about pneumonia, I will pick the first two sections and run it through the model prediction and see the category chosen.

#+BEGIN_SRC python

## Predict document class!

# https://en.wikipedia.org/wiki/Pneumonia

docs_new = ["""pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.[4][13] Typically symptoms include some combination of productive or dry cough, chest pain, fever, and trouble breathing.[2] Severity is variable.  Pneumonia is usually caused by infection with viruses or bacteria and less commonly by other microorganisms, certain medications and conditions such as autoimmune diseases.[4][5] Risk factors include other lung diseases such as cystic fibrosis, COPD, and asthma, diabetes, heart failure, a history of smoking, a poor ability to cough such as following a stroke, or a weak immune system.[6] Diagnosis is often based on the symptoms and physical examination.[7] Chest X-ray, blood tests, and culture of the sputum may help confirm the diagnosis.[7] The disease may be classified by where it was acquired with community, hospital, or health care associated pneumonia"""]
X_new_counts = count_vect.transform(docs_new) # Extract new doc features.
X_new_tfidf = tfidf_transformer.transform(X_new_counts)

predicted = clf.predict(X_new_tfidf)

for doc, category in zip(docs_new, predicted):
    print('%r => %s' % (doc, twenty_train.target_names[category]))

#+END_SRC

Now after running this ~pneumonia~ text we get from the model this prediction: 

~it was acquired with community, hospital, or health care associated pneumonia' => sci.med~ (science medical) so it got categorized as ~sci.med~ which is simply corret!

Now let's say a nother piece of text this time about ~abortion~ and see what the model will predict, here is the new text we have fed it with: https://en.wikipedia.org/wiki/Abortion the first section again which is:

> Abortion is the ending of pregnancy by removing an embryo or fetus before it can survive outside the uterus.[note 1] An abortion that occurs spontaneously is also known as a miscarriage. An abortion may be caused purposely and is then called an induced abortion, or less frequently, "induced miscarriage". The word abortion is often used to mean only induced abortions. A similar procedure after the fetus could potentially survive outside the womb is known as a "late termination of pregnancy"

And the resulting prediction by the model is:

~...survive outside the womb is known as a "late termination of pregnancy' => soc.religion.christian~

Which means that abortion was categorized as ~social religion christianity~ category => I don't know if to be happy, sad, depressed, or excited by this prediction.

**Summary of step 1**

It looks like there is a way to determine the class of an text snippet by it's content using machine learning models, for sure there are challenges but this appears to be rather well known problem and there are available methods for solving and optimizing it (changing model, parameters, better training input data).

Now for the next step we have expected that for each class/topic we are going to select the set of features which we are going to use for text summarization.  I'm afraid this part has to be manual, we have to say that for a topic "disease", the features are going to be a set of closed features suh as "mortality rate", "suspectible age group", "name", "average length".  And on the other hand for "cars" topic the summary template variables are going to be: "manufacturer", "engine type", "year", "color", "used/new", etc.  It appears like for these set of summary template variables are going to be hand crafted.

The question is for step 3, whether a model could extract the set of "variable values" from articles and apply a summary from them? I don't have the answer, at least not at my current googling phase.

Step 2 and 3 looks like lot of manual work, is it possible that I could do some googling for better and more automatic solutions or better approaches to this problem of summarization?

* Step 2 Extract Features

As we said in the previous section, extracting the relevant features for a topic is either a heavy manual work or magic-computer work.  You see, for every topic for every discussion there is its own unique set of feature, if its a luggage you have the dimentions, color, applies to low-cost or not, and ofcourse brand name for each of them.  I'm sure there must be a way out of it without programming the universe from scratch again.

After doing some more google search NER looks like a good candidate, at least for part of the problem.  NER? After doing some googling, I have noticed that NER seems like part of the solution, looking at ~spacy.io~ I see they have already implemented some common NER and have API to train new NER, standford NLP libraries also have an NER this time with java.

According to toward data science:

#+BEGIN_QUOTE
Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a sub-task of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc
#+END_QUOTE


Let's have a look at the abilities of ~spacy~ and what it can do for us and ccording to spacy's documentation:

#+BEGIN_QUOTE
The default model identifies a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new examples.
#+END_QUOTE

According to it's documentation it can identify the following (and not only) entities: ~PERSON, ORG (companies), PRODUCT, WORK_OF_ART (Books, ..), PERCENT, MONEY, QUANTITY, and a few more~

In addition it allws you to extend and train new models to recognize new entities.

Let's try it out with it's basic usage.

We start with their example:

#+BEGIN_SRC python

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

#+END_SRC

And when I run it I get:

#+BEGIN_SRC python

(u'Apple', 0, 5, u'ORG')
(u'U.K.', 27, 31, u'GPE')
(u'$1 billion', 44, 54, u'MONEY')

#+END_SRC

So it has recognized the company ~Apple~ the geogrpahical entity ~UK~ and a small amount of money: ~$1 billion~

Let's change the input sentence to: ~Google is looking at buying U.K. startup for $1 billion, if it works it might buy Apple~ and see that it identifies now two companies, there result of running the above code is:

#+BEGIN_SRC python

(u'Google', 0, 6, u'ORG')
(u'U.K.', 28, 32, u'GPE')
(u'$1 billion', 45, 55, u'MONEY')
(u'Apple', 84, 89, u'ORG')

#+END_SRC

What if I change from ~Apple~ to ~apple~ that is ~Google is looking at buying U.K. startup for $1 billion, if it works it might buy apple~

#+BEGIN_SRC python

(u'Google', 0, 6, u'ORG')
(u'U.K.', 28, 32, u'GPE')
(u'$1 billion', 45, 55, u'MONEY')

#+END_SRC

Aha so ~apple~ with lower case does not count as a company, what if google decides to eat an Apple? with upper case: ~Google is looking at buying U.K. startup for $1 billion, if it works it might eat an Apple~

#+BEGIN_SRC python

(u'Google', 0, 6, u'ORG')
(u'U.K.', 28, 32, u'GPE')
(u'$1 billion', 45, 55, u'MONEY')
(u'Apple', 85, 90, u'ORG')

#+END_SRC

It's a company apparently if Google decides to eat an Apples it's eating a company, interesting.

Let's take some arbitrary product from ebay and feed it into Spacy NER, so i'm taking *~Apple iPhone 8 4.7" Display 64GB UNLOCKED Smartphone US $499.99~* and let's see how spacy's NER parses it:

#+BEGIN_SRC python

(u'Apple iPhone 8 4.7', 0, 18, u'ORG')
(u'64', 28, 30, u'CARDINAL')
(u'UNLOCKED', 33, 41, u'PERSON')
(u'Smartphone', 42, 52, u'DATE')
(u'US', 53, 55, u'GPE')
(u'499.99', 57, 63, u'MONEY')

#+END_SRC

So the org was identified as ~Apple iPhone 8 4.7~ not so good i'm not aware of such a company it should have been a product, 64 was identieid as ~Cardinal~ this is good, ~UNLOCKED~ as a person, ~Smartphone~ as date, and ~US~ as geography, and 499.99 as money, this is partially good but definetly not satisfactory.

The good thing to remember is that spacy said they have a way to train new models so possibly with additional training for more domain specific items we could reach better results.



#+CAPTION: This is the caption for the next figure link (or table)
#+NAME:   fig:SED-HR4049
[[https://kanbanflow.com/img/avatars/22/man12.png]]
 

* Sentence Compression

While googling some more I've noticed there is another approach to text summarization called: "Sentence Compression", this approach is more compelling for me because from all the search results I get it looks like a fully automatic process (except for training).
Note that although we have text summarization there is another important topic called ~Sentence Compression~ in this case we are taking a rather small text and - compressing it, deleting undeeded words.

#+BEGIN_QUOTE
Sentence compression is a paraphrasing task where the goal is to generate sentences shorter than given while preserving the essential content

Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence. Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few).
#+END_QUOTE

References:

[[https://www.aclweb.org/anthology/D/D13/D13-1155.pdf][Overcoming the Lack of Parallel Data in Sentence Compression]]
[[https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43852.pdf][Sentence Compression by Deletion with LSTMs]]

* Resources



|-----------------------------------------------------------------+---------------------------------------------------------------------------------|
| resource                                                        | link                                                                            |
|-----------------------------------------------------------------+---------------------------------------------------------------------------------|
| Sentence Compression by Deletion with LSTMs                     | https://research.google.com/pubs/archive/43852.pdf                              |
| Models Zoo - Ready Made Models                                  | https://modelzoo.co/                                                            |
| A Neural Attention Model for Abstractive Sentence Summarization | https://arxiv.org/abs/1509.00685                                                |
| TensorFlow-Summarization                                        | https://github.com/thunlp/TensorFlow-Summarization                              |
| Webscrapper                                                     | http://webscraper.io/                                                           |
| Dzone on text summarization                                     | https://dzone.com/articles/a-guide-to-natural-language-processing-part-3        |
| DataSet                                                         | https://duc.nist.gov/duc2004/                                                   |
| Google Research DataSets for Sentence Compression               | https://github.com/google-research-datasets/sentence-compression                |
| How do I download DUC dataset for text summarization?           | https://www.quora.com/How-do-I-download-DUC-dataset-for-text-summarization      |
| **EXAMPLE**: Keras text summarization on news                   | https://github.com/chen0040/keras-text-summarization                            |
| Example: NLTK Simple Summarization                              | https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk        |
| Example: Text Summarize ROUGE scoring                           | http://forum.opennmt.net/t/text-summarization-on-gigaword-and-rouge-scoring/85  |
| SumBasic Clustering                                             | http://www.cs.middlebury.edu/~mpettit/project.html                              |
| Keras Text Classification                                       | https://medium.com/skyshidigital/getting-started-with-keras-624dbf106c87        |
| NLP for hackers TextRank for TextSummarization                  | https://nlpforhackers.io/textrank-text-summarization/                           |
| Track NLP Status and Progress - Summarization                   | https://github.com/sebastianruder/NLP-progress/blob/master/summarization.md     |
| Sentence Compression and Text Summarization - Many resources    | https://github.com/mathsyouth/awesome-text-summarization                        |
| Google AI Portal                                                | https://ai.google                                                               |
| Text Summarization Thesis                                       | https://tinyurl.com/text-summarization-thesis                                   |
| Text Compression Deletion Impl based on Katja Filippova Paper   | https://github.com/zhaohengyang/Generate-Parallel-Data-for-Sentence-Compression |
| Overcoming the Lack of Parallel Data in Sentence Compression    | https://www.aclweb.org/anthology/D/D13/D13-1155.pdf                             |
| Toward Data Science NER                                         | https://tinyurl.com/towarddatascience-ner                                                                                |
|                                                                 |                                                                                 |

* Ideas
extraction can also be good for us we just extract text

* Internal Resources

| Resource                                    | Link                                                                     |
| Machine Learning Mastery Text Summarization | https://machinelearningmastery.com/?s=text+summarization&submit=Search   |
| Identify Article Topic with keras           | https://medium.com/skyshidigital/getting-started-with-keras-624dbf106c87 |
|                                             |                                                                          |

* Ready made models
** https://github.com/tensorflow/models/tree/master/research/textsum

#+BEGIN_SRC bash

# With a preexisting conda installation.

conda install -c conda-forge tensorflow
conda install -c conda-forge bazel
cd tmp
git clone https://github.com/tensorflow/models
mkdir test-summarization
cd test-summarization
touch WORKSPACE
mkdir data
cp -r ~/tmp/models/research/textsum/data/data ./data/
cp -r ~/tmp/models/research/textsum/data/vocab ./data/
bazel build -c opt textsum/...

INFO: Analysed 7 targets (0 packages loaded).
INFO: Found 7 targets...
INFO: Elapsed time: 0.436s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action

bazel-bin/textsum/seq2seq_attention \
    --mode=train \
    --article_key=article \
    --abstract_key=abstract \
    --data_path=data/training-* \
    --vocab_path=data/vocab \
    --log_root=textsum/log_root \
    --train_dir=textsum/log_root/train



#+END_SRC


* On Rule Based Systems
Rule based systems are great, especially to start with, and if they satisfy you you can continue with them.  Some limitations however:

1. The number of options is limited by the rules you set, new ~words~ could appear in your domain, new items, how would you handle them with rule based, its only if it has a dynamic part.
1. What about errors meaning if someone wrote some text and had some typos, a rule based system is more strict, if someone wrote Apple and then Aplpe how would you know its the same brand with a rule based system?
